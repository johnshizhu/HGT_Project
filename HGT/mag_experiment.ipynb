{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Microsoft Academic Graph Dataset Experiment\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "This code opens the Microsoft Academic Graph Dataset and trains HGT\n",
    "Based on code provided by original HGT paper\n",
    "'''\n",
    "import torch\n",
    "from hgt import *\n",
    "from hgt_utils import *\n",
    "from local_access import *\n",
    "from ogb.nodeproppred import PygNodePropPredDataset\n",
    "from ogb.nodeproppred import Evaluator\n",
    "from graph import Graph\n",
    "import multiprocessing as mp\n",
    "import argparse\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import seaborn as sb\n",
    "\n",
    "print(\"Microsoft Academic Graph Dataset Experiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Data Preprocessing\n",
    "ogbn-mag only comes with paper node features, thus for other nodes types we take the average\n",
    "of connected paper nodes as input features. \n",
    "'''\n",
    "print(\"Begin Data Preprocessing\")\n",
    "print(\"\")\n",
    "print(\"Retrieving Data from Open Graph Benchmark ...\")\n",
    "\n",
    "# Get dataset using Pytorch Geometric Loader\n",
    "dataset = PygNodePropPredDataset(name='ogbn-mag')\n",
    "print(\"... Retrieval complete\")\n",
    "data = dataset[0] # pyg graph object\n",
    "evaluator = Evaluator(name='ogbn-mag')\n",
    "\n",
    "\n",
    "# Populating edge lists in Graph object based on edge_list\n",
    "print(\"Populating edge lists into Graph object\")\n",
    "edge_index_dict = data.edge_index_dict \n",
    "graph = Graph()\n",
    "edg = graph.edge_list\n",
    "years = data.node_year['paper'].t().numpy()[0]\n",
    "# for every type of edge relation i.e. ('author', 'affiliated_with', 'institution'), ...\n",
    "for key in edge_index_dict:\n",
    "    print(key) # print relation name\n",
    "    edges = edge_index_dict[key] \n",
    "    '''\n",
    "    tensor( [[      0,       1,       2,  ..., 1134645, 1134647, 1134648],\n",
    "             [    845,     996,    3197,  ...,    5189,    4668,    4668]]) example edges tensor\n",
    "    '''\n",
    "    # getting types of source, relation and edge ('author', 'affiliated_with', 'institution')\n",
    "    s_type, r_type, t_type = key[0], key[1], key[2]\n",
    "    elist = edg[t_type][s_type][r_type]\n",
    "    rlist = edg[s_type][t_type]['rev_' + r_type]\n",
    "    # adding year if the type is paper\n",
    "    for s_id, t_id in edges.t().tolist():\n",
    "        year = None\n",
    "        if s_type == 'paper':\n",
    "            year = years[s_id]\n",
    "        elif t_type == 'paper':\n",
    "            year = years[t_id]\n",
    "        elist[t_id][s_id] = year\n",
    "        rlist[s_id][t_id] = year\n",
    "\n",
    "# Reformatting edge list and computing node degrees\n",
    "print(\"Reformatting edge lists and computing node degrees\")\n",
    "edg = {}\n",
    "deg = {key : np.zeros(data.num_nodes_dict[key]) for key in data.num_nodes_dict}\n",
    "for k1 in graph.edge_list:\n",
    "    if k1 not in edg:\n",
    "        edg[k1] = {}\n",
    "    for k2 in graph.edge_list[k1]:\n",
    "        if k2 not in edg[k1]:\n",
    "            edg[k1][k2] = {}\n",
    "        for k3 in graph.edge_list[k1][k2]:\n",
    "            if k3 not in edg[k1][k2]:\n",
    "                edg[k1][k2][k3] = {}\n",
    "            for e1 in graph.edge_list[k1][k2][k3]:\n",
    "                if len(graph.edge_list[k1][k2][k3][e1]) == 0:\n",
    "                    continue\n",
    "\n",
    "                edg[k1][k2][k3][e1] = {}\n",
    "                for e2 in graph.edge_list[k1][k2][k3][e1]:\n",
    "                    edg[k1][k2][k3][e1][e2] = graph.edge_list[k1][k2][k3][e1][e2]\n",
    "                deg[k1][e1] += len(edg[k1][k2][k3][e1])\n",
    "            print(k1, k2, k3, len(edg[k1][k2][k3]))\n",
    "graph.edge_list = edg # inserting new edge list into Graph object\n",
    "\n",
    "# Constructing node feature vectors for each node type in graph\n",
    "print(\"Constructing node feature vectors for each node type in graph\")\n",
    "paper_node_features = data.x_dict['paper'].numpy() # data into numpy\n",
    "# append log degree to get full paper node features\n",
    "graph.node_feature['paper'] = np.concatenate((paper_node_features, np.log10(deg['paper'].reshape(-1, 1))), axis=-1)\n",
    "# These are node types: {'author': 1134649, 'field_of_study': 59965, 'institution': 8740, 'paper': 736389}\n",
    "for node_type in data.num_nodes_dict:\n",
    "    print(node_type)\n",
    "    if node_type not in ['paper', 'institution']:\n",
    "        i = []\n",
    "        for rel_type in graph.edge_list[node_type]['paper']:\n",
    "            for t in graph.edge_list[node_type]['paper'][rel_type]:\n",
    "                for s in graph.edge_list[node_type]['paper'][rel_type][t]:\n",
    "                    i += [[t,s]]\n",
    "            if len(i) == 0:\n",
    "                continue\n",
    "        i = np.array(i).T\n",
    "        v = np.ones(i.shape[1])\n",
    "        m = normalize(sp.coo_matrix((v, i), \\\n",
    "            shape=(data.num_nodes_dict[node_type], data.num_nodes_dict['paper'])))\n",
    "        out = m.dot(paper_node_features)\n",
    "        graph.node_feature[node_type] = np.concatenate((out, np.log10(deg[node_type].reshape(-1, 1))), axis=-1)\n",
    "\n",
    "# Contructing node feature vectors for institution nodes\n",
    "print(\"Constructing Node features for institutions\")    \n",
    "cv = graph.node_feature['author'][:, :-1]\n",
    "i = []\n",
    "for _rel in graph.edge_list['institution']['author']:\n",
    "    for j in graph.edge_list['institution']['author'][_rel]:\n",
    "        for t in graph.edge_list['institution']['author'][_rel][j]:\n",
    "            i += [[j, t]]\n",
    "i = np.array(i).T\n",
    "v = np.ones(i.shape[1])\n",
    "m = normalize(sp.coo_matrix((v, i), \\\n",
    "    shape=(data.num_nodes_dict['institution'], data.num_nodes_dict['author'])))\n",
    "out = m.dot(cv)\n",
    "graph.node_feature['institution'] = np.concatenate((out, np.log10(deg['institution'].reshape(-1, 1))), axis=-1)      \n",
    "\n",
    "# y_dict\n",
    "y = data.y_dict['paper'].t().numpy()[0]\n",
    "\n",
    "# Splitting dataset into training, validation and testing\n",
    "print(\"Splitting dataset into train, val and test\")\n",
    "split_idx = dataset.get_idx_split()\n",
    "train_paper = split_idx['train']['paper'].numpy()\n",
    "valid_paper = split_idx['valid']['paper'].numpy()\n",
    "test_paper  = split_idx['test']['paper'].numpy()\n",
    "\n",
    "graph.y = y\n",
    "graph.train_paper = train_paper\n",
    "graph.valid_paper = valid_paper\n",
    "graph.test_paper  = test_paper\n",
    "graph.years       = years\n",
    "\n",
    "print(\"Creating Masks\")\n",
    "graph.train_mask = np.zeros(len(graph.node_feature['paper']), dtype=bool)\n",
    "graph.train_mask[graph.train_paper] = True\n",
    "\n",
    "graph.valid_mask = np.zeros(len(graph.node_feature['paper']), dtype=bool)\n",
    "graph.valid_mask[graph.valid_paper] = True\n",
    "\n",
    "graph.test_mask = np.zeros(len(graph.node_feature['paper']),  dtype=bool)\n",
    "graph.test_mask[graph.test_paper] = True\n",
    "\n",
    "# Preprocessing graph object is now complete\n",
    "print(\"Preprocessing complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Creating Model\n",
    "'''\n",
    "print(\"Creating Model\")\n",
    "hgt_GNN = HGTModel(len(graph.node_feature['paper'][0]), # input_dim\n",
    "                   256,                                 # hidden_dim\n",
    "                   len(graph.get_types()),              # num_node_types\n",
    "                   len(graph.get_meta_graph()),         # num_edge_types\n",
    "                   8,                                   # num_heads\n",
    "                   4,                                   # num_layers\n",
    "                   0.2,                                 # dropout\n",
    "                   prev_norm = True,                    # normalization on all but last layer\n",
    "                   last_norm = False,                   # normalization on last layer\n",
    "                   use_rte = False)                     # use relative temporal encoding \n",
    "classifier = Classifier(256, graph.y.max()+1)\n",
    "\n",
    "HGT_classifier = nn.Sequential(hgt_GNN, classifier)\n",
    "\n",
    "print(HGT_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Preprocessing data\n",
    "\n",
    "'''\n",
    "batch_number = 32 # number of sampled graphs for each epoch\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "num_workers = 8\n",
    "\n",
    "clip = 1.0\n",
    "\n",
    "sample_depth = 6\n",
    "\n",
    "sample_width = 520\n",
    "\n",
    "plot = False # True or false to plot data\n",
    "\n",
    "save_directory = r\"C:\\Users\\johns\\OneDrive\\Desktop\\HGT_Data\\models\"\n",
    "# device = torch.device() DEVICE CONTROL POSSIBLE\n",
    "target_nodes = np.arange(len(graph.node_feature['paper']))\n",
    "\n",
    "# Negative Log Likelihood Loss\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Get list of model parameters w/ associated names\n",
    "parameters_optimizer = list(HGT_classifier.named_parameters())\n",
    "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in parameters_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
    "    {'params': [p for n, p in parameters_optimizer if any(nd in n for nd in no_decay)],     'weight_decay': 0.0}\n",
    "]\n",
    "\n",
    "# AdamW optimizer w/specified parameter groups and epsilon value\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, eps=1e-06)\n",
    "\n",
    "# Create a OneCycleLR learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, pct_start=0.05, anneal_strategy='linear', final_div_factor=10,\\\n",
    "                        max_lr = 5e-4, total_steps = batch_size * num_epochs + 1)\n",
    "\n",
    "stats = []\n",
    "result = []\n",
    "best_val = 0\n",
    "training_step = 0\n",
    "\n",
    "def randint():\n",
    "    return np.random.randint(2**31 - 1)\n",
    "\n",
    "def ogbn_sample(seed, samp_nodes):\n",
    "    np.random.seed(seed)\n",
    "    ylabel      = torch.LongTensor(graph.y[samp_nodes])\n",
    "    feature, times, edge_list, indxs, _ = sample_subgraph(graph, \\\n",
    "                inp = {'paper': np.concatenate([samp_nodes, graph.years[samp_nodes]]).reshape(2, -1).transpose()}, \\\n",
    "                sampled_depth = sample_depth, sampled_number = sample_width, \\\n",
    "                    feature_extractor = feature_MAG)\n",
    "    node_feature, node_type, edge_time, edge_index, edge_type, node_dict, edge_dict = \\\n",
    "            to_torch(feature, times, edge_list, graph)\n",
    "    train_mask = graph.train_mask[indxs['paper']]\n",
    "    valid_mask = graph.valid_mask[indxs['paper']]\n",
    "    test_mask  = graph.test_mask[indxs['paper']]\n",
    "    ylabel     = graph.y[indxs['paper']]\n",
    "    return node_feature, node_type, edge_time, edge_index, edge_type, (train_mask, valid_mask, test_mask), ylabel\n",
    "\n",
    "def prepare_data(pool, task_type = 'train', s_idx = 0, n_batch = batch_number, batch_size = batch_size):\n",
    "    '''\n",
    "        Sampled and prepare training and validation data using multi-process parallization.\n",
    "    '''\n",
    "    jobs = []\n",
    "    if task_type == 'train':\n",
    "        for batch_id in np.arange(n_batch):\n",
    "            print(f'starting preprocessing batch: {batch_id}')\n",
    "            p = pool.apply_async(ogbn_sample, args=([randint(), \\\n",
    "                            np.random.choice(target_nodes, batch_size, replace = False)]))\n",
    "            jobs.append(p)\n",
    "            print(f'finished preprocessing batch: {batch_id}')\n",
    "    elif task_type == 'sequential':\n",
    "        for i in np.arange(n_batch):\n",
    "            target_papers = graph.test_paper[(s_idx + i) * batch_size : (s_idx + i + 1) * batch_size]\n",
    "            p = pool.apply_async(ogbn_sample, args=([randint(), target_papers]))\n",
    "            jobs.append(p)\n",
    "    elif task_type == 'variance_reduce':\n",
    "        target_papers = graph.test_paper[s_idx * batch_size : (s_idx + 1) * batch_size]\n",
    "        for batch_id in np.arange(n_batch):\n",
    "            p = pool.apply_async(ogbn_sample, args=([randint(), target_papers]))\n",
    "            jobs.append(p)\n",
    "    print(\"Preprocessing complete ---------------------------------------------------\")\n",
    "    return jobs\n",
    "\n",
    "pool = mp.Pool(num_workers)\n",
    "\n",
    "# start time\n",
    "start_time = time.time()\n",
    "jobs = prepare_data(pool)\n",
    "\n",
    "# Begin Training\n",
    "print(\"Beginning Training\")\n",
    "print(\"\")\n",
    "train_step = 0\n",
    "epoch_tracker = 1\n",
    "print(\"starting loop\")\n",
    "\n",
    "print(f'length of jobs is: {len(jobs)}')\n",
    "print(f'type of jobs is: {type}')\n",
    "\n",
    "for epoch in np.arange(num_epochs) + 1:\n",
    "    print(f\"epoch: {epoch_tracker}\")\n",
    "    '''\n",
    "        Prepare Training and Validation Data\n",
    "    '''\n",
    "    datas = [job.get() for job in jobs]\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    '''\n",
    "        After the data is collected, close the pool and then reopen it.\n",
    "    '''\n",
    "    pool = mp.Pool(num_workers)\n",
    "    jobs = prepare_data(pool)\n",
    "    et = time.time()\n",
    "    print('Data Preparation: %.1fs' % (et - st))\n",
    "    \n",
    "    '''\n",
    "        Train\n",
    "    '''\n",
    "    print(\"Training\")\n",
    "    HGT_classifier.train()\n",
    "    stat = []\n",
    "    for node_feature, node_type, edge_time, edge_index, edge_type, (train_mask, valid_mask, test_mask), ylabel in datas:\n",
    "        node_rep = HGT_classifier.forward(node_feature, node_type, \\\n",
    "                               edge_time, edge_index, edge_type)\n",
    "        ylabel = torch.LongTensor(ylabel)\n",
    "        train_res  = classifier.forward(node_rep[:len(ylabel)][train_mask])\n",
    "        valid_res  = classifier.forward(node_rep[:len(ylabel)][valid_mask])\n",
    "        test_res   = classifier.forward(node_rep[:len(ylabel)][test_mask])\n",
    "\n",
    "        train_loss = criterion(train_res, ylabel[train_mask])\n",
    "\n",
    "        optimizer.zero_grad() \n",
    "        train_loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(HGT_classifier.parameters(), clip)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_step += 1\n",
    "        scheduler.step(train_step)\n",
    "\n",
    "        train_acc  = evaluator.eval({\n",
    "                        'y_true': ylabel[train_mask].unsqueeze(-1),\n",
    "                        'y_pred': train_res.argmax(dim=1).unsqueeze(-1)\n",
    "                    })['acc']\n",
    "        valid_acc  = evaluator.eval({\n",
    "                        'y_true': ylabel[valid_mask].unsqueeze(-1),\n",
    "                        'y_pred': valid_res.argmax(dim=1).unsqueeze(-1)\n",
    "                    })['acc']\n",
    "        test_acc   = evaluator.eval({\n",
    "                        'y_true': ylabel[test_mask].unsqueeze(-1),\n",
    "                        'y_pred': test_res.argmax(dim=1).unsqueeze(-1)\n",
    "                    })['acc']\n",
    "        stat += [[train_loss.item(), train_acc, valid_acc, test_acc]]\n",
    "        del node_rep, train_loss, ylabel\n",
    "    stats += [stat]\n",
    "    avgs = np.average(stat, axis=0)\n",
    "    if avgs[2] > best_val:\n",
    "        best_val = avgs[2]\n",
    "        torch.save(HGT_classifier.state_dict(), save_directory)\n",
    "        print('UPDATE!!!  ' + str(best_val))\n",
    "    print('Epoch: %d LR: %.5f Train Loss: %.4f Train Acc: %.4f Valid Acc: %.4f Test Acc: %.4f' % \\\n",
    "         (epoch,  optimizer.param_groups[0]['lr'], avgs[0], avgs[1], avgs[2], avgs[3]))\n",
    "    st = time.time()\n",
    "    if plot and epoch % 50 == 0:\n",
    "        s = np.concatenate(stats)\n",
    "        for i in range(4):\n",
    "            data = np.stack((s[-batch_number * 100:, i], np.arange(len(s[-batch_number * 100:, i])) // batch_number), axis=0).transpose()\n",
    "            sb.lineplot(data = pd.DataFrame(data, columns = ['Value', 'Epoch']), x='Epoch', y='Value')\n",
    "            plt.show()\n",
    "            \n",
    "if plot:\n",
    "    s = np.concatenate(stats)\n",
    "    for i in range(4):\n",
    "        data = np.stack((s[batch_number * 100:, i], np.arange(len(s[batch_number * 100:, i])) // batch_number), axis=0).transpose()\n",
    "        sb.lineplot(data = pd.DataFrame(data, columns = ['Value', 'Epoch']), x='Epoch', y='Value')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
